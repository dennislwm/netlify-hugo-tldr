---
author: "Dennis Lee ðŸ‘¨"
title: "Comparison of YouTube Free Autogenerated Subtitles vs Paid AI"
date: "Fri, 19 Jan 2024 11:22:9 +0800"
description: "Comparison of Youtube Free Autogenerated Subtitles vs Paid Ai"
draft: true
hideToc: false
enableToc: true
enableTocContent: true
authorEmoji: ðŸ‘¨
---

<!-- TOC depthfrom:2 -->

- [TL;DR](#tldr)
- [Prerequisites](#prerequisites)
- [Create a process to download and convert an autogenerated subtitle file from YouTube](#create-a-process-to-download-and-convert-an-autogenerated-subtitle-file-from-youtube)
- [Comparison of YouTube vs Paid AI](#comparison-of-youtube-vs-paid-ai)
- [Conclusion](#conclusion)
- [Get the Source Code](#get-the-source-code)
- [What To Do Next](#what-to-do-next)

<!-- /TOC -->

---
## TL;DR

[YouTube](https://youtube.com) isn't just for watching and sharing videos. It's also for listening to podcasts, and reading transcriptions. Using an open-source application gives you the ability to download the video, audio or text data only, where you can consume the data at your own leisure, or even autogenerate blog articles.

In this first part of a multi-series blog you'll use a command-line interface (CLI) `yt-dlp` to download YouTube's free autogenerated subtitles and compare it to paid AI transcription services, such as [Deepgram](https://deepgram.com), or [OpenAI](https://openai.com).

---
## Prerequisites

* An open-source CLI [yt-dlp](https://github.com/yt-dlp/yt-dlp) to download YouTube files.
* [Python 3](https://www.python.org)

---
## Create a process to download and convert an autogenerated subtitle file from YouTube

In order to design a workflow to autogenerate blog articles, we must first create a process to download a subtitle file from YouTube.

1. Open a terminal and run the command `yt-dlp --version` to check if it is installed.

```sh
2023.10.1
```

2. Type the following command to download the autogenerated subtitle file of a YouTube video. Replace the YOUTUBE_URL with any YouTube link, e.g. `https://www.youtube.com/watch?v=x3vnCKivCjs`.

```sh
yt-dlp --write-auto-sub --skip-download [YOUTUBE_URL]
```

You should see an output similar to below.

```sh
[youtube] Extracting URL: https://www.youtube.com/watch?v=x3vnCKivCjs
[youtube] x3vnCKivCjs: Downloading webpage
[youtube] x3vnCKivCjs: Downloading ios player API JSON
[youtube] x3vnCKivCjs: Downloading android player API JSON
[youtube] x3vnCKivCjs: Downloading m3u8 information
[info] x3vnCKivCjs: Downloading subtitles: en
[info] x3vnCKivCjs: Downloading 1 format(s): 22
[info] Writing video subtitles to: The Fastest Way to Lose Belly Fat [x3vnCKivCjs].en.vtt
[download] Destination: The Fastest Way to Lose Belly Fat [x3vnCKivCjs].en.vtt
[download] 100% of   85.84KiB in 00:00:00 at 879.88KiB/s
```

The subtitle file name is created using the video title and code, e.g. `The Fastest Way to Lose Belly Fat [x3vnCKivCjs].en.vtt`.

The reason that you'll need to convert the subtitle file is if you open the `vtt` file, it contains metatags that makes it hard to read.

Fortunately, there is a [Python script](https://gist.github.com/glasslion/b2fcad16bc8a9630dbd7a945ab5ebf5e) that converts youtube subtitle file (`vtt`) to plain text. Credit to [glasslion](https://gist.github.com/glasslion) for making it open-source.

3. Download the above Python script `vtt2text.py` into the same folder as your `vtt` file.

4. In your terminal, type the following command.

```sh
python vtt2text.py The\ Fastest\ Way\ to\ Lose\ Belly\ Fat\ \[x3vnCKivCjs\].en.vtt
```

5. If successful, you'll find a new `txt` file created, e.g. `The Fastest Way to Lose Belly Fat [x3vnCKivCjs].en.txt`, which has readable text but still contains a few metadata lines.

```txt
00:00
today I'm going to share with you the absolute fastest way to lose your belly
now you could have the best willpower the best discipline really want it
really bad and never really see any results because you're missing the
technique you're missing the strategy I'm the perfect example I took guitar
lessons for six years right as a teenager and I never really progressed
or never really went anywhere because the techniques that were taught to me
were just not that great great the same thing happened with tennis in college I
...
```

> Note: You'll need to tweak the Python script to remove the metadata lines. If you prefer to download my enhanced Python script, you can [Get the Source Code](#get-the-source-code).

---
## Comparison of YouTube vs Paid AI

Let's start with the negatives.

|                    YouTube free autogenerated subtitles                    | Paid AI transcription services |
|:--------------------------------------------------------------------------:|:------------------------------:|
|         1. Must be familiar or comfortable using the command-line.         |                                |
| 2. Two-step process of downloading the subtitle and converting it to text. |                                |
|             3. The converted text has a few lines of metadata.             |                                |
|                 4. The converted text has no punctuations.                 |                                |

For the positives.

|                         YouTube free autogenerated subtitles                         | Paid AI transcription services |
|:------------------------------------------------------------------------------------:|:------------------------------:|
|        1. You can create a workflow of automated processes using a pipeline.         |                                |
| 2. No subscription cost, only your Internet bandwidth for downloading the subtitles. |                                |

---
## Conclusion

---
## Get the Source Code


You can download the above source code from my GitHub repository [dennislwm/playscribe](https://github.com/dennislwm/playscribe).

---
## What To Do Next

You can further extend your code in several meaningful ways:

* Implement a GitHub Actions that will continuously monitor an RSS feed for new YouTube links, and process the links and returns the transcribe results.

* Implement a static site blog that publishes each transcribe result as a new post, with an RSS feed to allow users to subscribe to new posts.

* * *

**Was this article useful? Help me to improve by replying in the comments.**

[![Buy Me A Coffee donate button](https://img.shields.io/badge/buy%20me%20a%20coffee-donate-yellow.svg)](https://ko-fi.com/dennislwm "Donate to this project using Buy Me A Coffee")
